{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e4ea0b",
   "metadata": {},
   "source": [
    "# TOR html content ai or human detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.20.1-py3-none-any.whl (16 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m369.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.1/447.1 kB\u001b[0m \u001b[31m298.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m791.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.0/174.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2025.11.3-cp310-cp310-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m970.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp310-cp310-macosx_10_9_universal2.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2025.11.12 charset_normalizer-3.4.4 filelock-3.20.1 fsspec-2025.12.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 numpy-2.2.6 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 urllib3-2.6.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting tensorflow<2.21,>=2.20\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-macosx_12_0_arm64.whl (200.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.4/200.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-2.0.1-cp310-cp310-macosx_11_0_arm64.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m535.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.26.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.6)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard~=2.20.0\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting protobuf>=5.28.0\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.6/427.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml_dtypes<1.0.0,>=0.5.1\n",
      "  Downloading ml_dtypes-0.5.4-cp310-cp310-macosx_10_9_universal2.whl (679 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.7/679.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras>=3.10.0\n",
      "  Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.15.1-cp310-cp310-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Collecting opt_einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.6.6 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Collecting google_pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m463.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.18.0-cp310-cp310-macosx_11_0_arm64.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.5/330.5 kB\u001b[0m \u001b[31m827.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m969.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.11.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading pillow-12.0.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting markupsafe>=2.1.1\n",
      "  Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m498.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, termcolor, tensorboard-data-server, protobuf, pillow, optree, opt_einsum, ml_dtypes, mdurl, markupsafe, markdown, h5py, grpcio, google_pasta, gast, absl-py, werkzeug, markdown-it-py, astunparse, tensorboard, rich, keras, tensorflow, tf-keras\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 pillow-12.0.0 protobuf-6.33.2 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 tf-keras-2.20.1 werkzeug-3.1.4 wheel-0.45.1 wrapt-2.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# used pyenv 3/10/15 env \n",
    "!pip install transformers\n",
    "!pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039726be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m657.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting soupsieve>=1.6.1\n",
      "  Downloading soupsieve-2.8.1-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.14.3 bs4-0.0.2 soupsieve-2.8.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbcbc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from torch) (2025.12.0)\n",
      "Collecting networkx>=2.5.1\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Installing collected packages: mpmath, sympy, networkx, jinja2, torch\n",
      "Successfully installed jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ۱. تنظیمات مسیر و پروکسی\n",
    "FILE_PATH = \"/Users/kiananasiri/Downloads/TorBot/collected_data_v10/page3.html\"\n",
    "OUTPUT_DIR = \"html_dataset_100\"\n",
    "TARGET_COUNT = 100\n",
    "proxies = {\n",
    "    'http': 'socks5h://127.0.0.1:9050',\n",
    "    'https': 'socks5h://127.0.0.1:9050'\n",
    "}\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# ۲. خواندن فایل و استخراج لینک‌ها\n",
    "def get_urls_from_file(path):\n",
    "    print(f\"Reading links from: {path}\")\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "            # استخراج تمام متن‌هایی که با http شروع شده و شامل .onion هستند\n",
    "            # (چون فایل شما لینک‌ها را به صورت متن ساده یا داخل تگ <a> دارد)\n",
    "            all_text = soup.get_text(separator=' ')\n",
    "            words = all_text.split()\n",
    "            for word in words:\n",
    "                if \".onion\" in word and word.startswith(\"http\"):\n",
    "                    # تمیز کردن کاراکترهای اضافه مثل <br> یا فضای خالی\n",
    "                    clean_url = word.strip().replace('<br>', '').replace(',', '')\n",
    "                    if clean_url not in urls:\n",
    "                        urls.append(clean_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "    return urls\n",
    "\n",
    "# ۳. تابع دانلود تکی\n",
    "def download_site(url, index):\n",
    "    try:\n",
    "        print(f\"[{index}] Fetching: {url}\")\n",
    "        r = requests.get(url, proxies=proxies, timeout=25, verify=False)\n",
    "        if r.status_code == 200 and len(r.text) > 100:\n",
    "            filename = f\"site_{index}.html\"\n",
    "            with open(os.path.join(OUTPUT_DIR, filename), \"w\", encoding='utf-8') as f:\n",
    "                f.write(r.text)\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# ۴. اجرای اصلی\n",
    "if __name__ == \"__main__\":\n",
    "    all_urls = get_urls_from_file(FILE_PATH)\n",
    "    print(f\"Found {len(all_urls)} onion links in your file.\")\n",
    "    \n",
    "    # محدود کردن به لینک‌های موجود یا ۱۰۰ عدد\n",
    "    urls_to_crawl = all_urls[:200] # ۲۰۰ تا برمی‌داریم که حتماً ۱۰۰ تا موفق داشته باشیم\n",
    "    \n",
    "    success_count = 0\n",
    "    # استفاده از ThreadPool برای سرعت (۱۰ دانلود همزمان)\n",
    "    print(\"Starting Multi-threaded download...\")\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # ارسال لینک‌ها به تردها\n",
    "        results = list(executor.map(lambda p: download_site(p[1], p[0]), enumerate(urls_to_crawl)))\n",
    "        success_count = sum(1 for x in results if x)\n",
    "\n",
    "    print(f\"\\n--- DONE ---\")\n",
    "    print(f\"Total links processed: {len(urls_to_crawl)}\")\n",
    "    print(f\"Successfully saved: {success_count} files in '{OUTPUT_DIR}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a380e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b1f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Neural Network (RoBERTa)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Loading Neural Network (RoBERTa)...\")\n",
    "detector = pipeline(\"text-classification\", model=\"roberta-base-openai-detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiananasiri/.pyenv/versions/3.10.15/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Neural Network (RoBERTa)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 168 files...\n",
      "File: site_92.html | Prediction: Human (99.98%)\n",
      "File: site_147.html | Prediction: Human (98.56%)\n",
      "File: site_110.html | Prediction: Human (98.67%)\n",
      "File: site_84.html | Prediction: Human (99.98%)\n",
      "File: site_151.html | Prediction: Human (99.98%)\n",
      "File: site_10.html | Prediction: Human (99.97%)\n",
      "File: site_47.html | Prediction: Human (99.93%)\n",
      "File: site_51.html | Prediction: AI (53.64%)\n",
      "File: site_184.html | Prediction: Human (99.30%)\n",
      "File: site_71.html | Prediction: Human (99.98%)\n",
      "File: site_26.html | Prediction: Human (96.75%)\n",
      "File: site_67.html | Prediction: Human (98.96%)\n",
      "File: site_126.html | Prediction: Human (95.90%)\n",
      "File: site_171.html | Prediction: Human (99.98%)\n",
      "File: site_167.html | Prediction: Human (99.73%)\n",
      "File: site_188.html | Prediction: AI (88.38%)\n",
      "File: site_130.html | Prediction: Human (99.91%)\n",
      "File: site_131.html | Prediction: Human (98.75%)\n",
      "File: site_166.html | Prediction: Human (99.88%)\n",
      "File: site_170.html | Prediction: Human (99.98%)\n",
      "File: site_127.html | Prediction: Human (99.98%)\n",
      "File: site_66.html | Prediction: Human (99.98%)\n",
      "File: site_5.html | Prediction: AI (88.16%)\n",
      "File: site_89.html | Prediction: AI (50.17%)\n",
      "File: site_31.html | Prediction: AI (99.92%)\n",
      "File: site_27.html | Prediction: Human (96.75%)\n",
      "File: site_185.html | Prediction: Human (99.98%)\n",
      "File: site_50.html | Prediction: Human (99.92%)\n",
      "File: site_193.html | Prediction: Human (99.62%)\n",
      "File: site_46.html | Prediction: Human (99.93%)\n",
      "File: site_150.html | Prediction: Human (83.46%)\n",
      "File: site_85.html | Prediction: Human (99.98%)\n",
      "File: site_111.html | Prediction: Human (99.98%)\n",
      "File: site_146.html | Prediction: Human (99.51%)\n",
      "File: site_93.html | Prediction: Human (99.92%)\n",
      "File: site_77.html | Prediction: Human (99.97%)\n",
      "File: site_98.html | Prediction: Human (95.99%)\n",
      "File: site_36.html | Prediction: Human (99.62%)\n",
      "File: site_61.html | Prediction: Human (99.92%)\n",
      "File: site_120.html | Prediction: Human (74.14%)\n",
      "File: site_177.html | Prediction: Human (99.97%)\n",
      "File: site_198.html | Prediction: Human (98.63%)\n",
      "File: site_161.html | Prediction: Human (99.84%)\n",
      "File: site_136.html | Prediction: Human (99.97%)\n",
      "File: site_94.html | Prediction: Human (99.62%)\n",
      "File: site_141.html | Prediction: AI (99.92%)\n",
      "File: site_116.html | Prediction: Human (95.99%)\n",
      "File: site_82.html | Prediction: Human (51.64%)\n",
      "File: site_157.html | Prediction: Human (99.98%)\n",
      "File: site_41.html | Prediction: Human (94.28%)\n",
      "File: site_194.html | Prediction: Human (99.22%)\n",
      "File: site_57.html | Prediction: Human (99.98%)\n",
      "File: site_182.html | Prediction: Human (98.96%)\n",
      "File: site_183.html | Prediction: Human (99.96%)\n",
      "File: site_56.html | Prediction: Human (99.92%)\n",
      "File: site_195.html | Prediction: Human (99.59%)\n",
      "File: site_40.html | Prediction: Human (99.98%)\n",
      "File: site_156.html | Prediction: Human (99.97%)\n",
      "File: site_83.html | Prediction: Human (98.56%)\n",
      "File: site_101.html | Prediction: Human (99.97%)\n",
      "File: site_117.html | Prediction: Human (99.98%)\n",
      "File: site_140.html | Prediction: AI (99.92%)\n",
      "File: site_137.html | Prediction: Human (75.63%)\n",
      "File: site_176.html | Prediction: Human (99.98%)\n",
      "File: site_199.html | Prediction: Human (99.98%)\n",
      "File: site_121.html | Prediction: Human (75.63%)\n",
      "File: site_60.html | Prediction: Human (99.98%)\n",
      "File: site_3.html | Prediction: AI (99.98%)\n",
      "File: site_37.html | Prediction: Human (99.62%)\n",
      "File: site_76.html | Prediction: Human (99.94%)\n",
      "File: site_99.html | Prediction: Human (95.99%)\n",
      "File: site_134.html | Prediction: AI (99.77%)\n",
      "File: site_59.html | Prediction: Human (97.63%)\n",
      "File: site_175.html | Prediction: Human (98.43%)\n",
      "File: site_122.html | Prediction: Human (99.98%)\n",
      "File: site_63.html | Prediction: Human (99.87%)\n",
      "File: site_159.html | Prediction: Human (98.56%)\n",
      "File: site_34.html | Prediction: Human (99.17%)\n",
      "File: site_118.html | Prediction: Human (99.98%)\n",
      "File: site_75.html | Prediction: Human (99.97%)\n",
      "File: site_138.html | Prediction: Human (99.41%)\n",
      "File: site_180.html | Prediction: Human (95.49%)\n",
      "File: site_55.html | Prediction: Human (99.58%)\n",
      "File: site_196.html | Prediction: Human (99.62%)\n",
      "File: site_179.html | Prediction: Human (99.98%)\n",
      "File: site_43.html | Prediction: Human (99.12%)\n",
      "File: site_155.html | Prediction: AI (81.78%)\n",
      "File: site_80.html | Prediction: Human (99.98%)\n",
      "File: site_102.html | Prediction: Human (99.98%)\n",
      "File: site_38.html | Prediction: AI (74.66%)\n",
      "File: site_114.html | Prediction: Human (99.82%)\n",
      "File: site_96.html | Prediction: AI (99.77%)\n",
      "File: site_79.html | Prediction: Human (99.98%)\n",
      "File: site_97.html | Prediction: AI (50.17%)\n",
      "File: site_78.html | Prediction: Human (99.47%)\n",
      "File: site_142.html | Prediction: Human (99.88%)\n",
      "File: site_81.html | Prediction: Human (99.98%)\n",
      "File: site_154.html | Prediction: Human (99.98%)\n",
      "File: site_42.html | Prediction: Human (99.65%)\n",
      "File: site_197.html | Prediction: Human (96.57%)\n",
      "File: site_178.html | Prediction: AI (88.11%)\n",
      "File: site_54.html | Prediction: Human (99.90%)\n",
      "File: site_181.html | Prediction: Human (99.98%)\n",
      "File: site_139.html | Prediction: Human (98.68%)\n",
      "File: site_74.html | Prediction: AI (93.09%)\n",
      "File: site_119.html | Prediction: Human (99.98%)\n",
      "File: site_35.html | Prediction: Human (99.93%)\n",
      "File: site_158.html | Prediction: Human (99.98%)\n",
      "File: site_62.html | Prediction: Human (99.33%)\n",
      "File: site_123.html | Prediction: AI (88.11%)\n",
      "File: site_174.html | Prediction: Human (99.86%)\n",
      "File: site_162.html | Prediction: Human (99.98%)\n",
      "File: site_58.html | Prediction: Human (99.98%)\n",
      "File: site_135.html | Prediction: Human (99.98%)\n",
      "File: site_186.html | Prediction: Human (85.65%)\n",
      "File: site_53.html | Prediction: Human (97.10%)\n",
      "File: site_190.html | Prediction: Human (99.04%)\n",
      "File: site_45.html | Prediction: Human (99.90%)\n",
      "File: site_128.html | Prediction: Human (99.98%)\n",
      "File: site_153.html | Prediction: Human (95.49%)\n",
      "File: site_86.html | Prediction: Human (99.98%)\n",
      "File: site_69.html | Prediction: Human (99.91%)\n",
      "File: site_104.html | Prediction: AI (50.17%)\n",
      "File: site_112.html | Prediction: Human (99.98%)\n",
      "File: site_28.html | Prediction: Human (99.98%)\n",
      "File: site_145.html | Prediction: AI (85.94%)\n",
      "File: site_90.html | Prediction: Human (99.97%)\n",
      "File: site_132.html | Prediction: Human (99.88%)\n",
      "File: site_165.html | Prediction: Human (99.98%)\n",
      "File: site_49.html | Prediction: Human (99.98%)\n",
      "File: site_173.html | Prediction: Human (99.93%)\n",
      "File: site_124.html | Prediction: Human (99.98%)\n",
      "File: site_65.html | Prediction: Human (98.09%)\n",
      "File: site_108.html | Prediction: Human (99.69%)\n",
      "File: site_24.html | Prediction: Human (99.98%)\n",
      "File: site_73.html | Prediction: Human (95.49%)\n",
      "File: site_149.html | Prediction: Human (99.97%)\n",
      "File: site_148.html | Prediction: Human (99.98%)\n",
      "File: site_72.html | Prediction: AI (98.25%)\n",
      "File: site_25.html | Prediction: Human (97.29%)\n",
      "File: site_109.html | Prediction: Human (98.62%)\n",
      "File: site_33.html | Prediction: Human (99.62%)\n",
      "File: site_64.html | Prediction: Human (99.98%)\n",
      "File: site_125.html | Prediction: Human (87.70%)\n",
      "File: site_172.html | Prediction: Human (99.98%)\n",
      "File: site_48.html | Prediction: Human (96.75%)\n",
      "File: site_133.html | Prediction: Human (99.82%)\n",
      "File: site_91.html | Prediction: Human (99.98%)\n",
      "File: site_144.html | Prediction: Human (99.98%)\n",
      "File: site_29.html | Prediction: AI (99.77%)\n",
      "File: site_113.html | Prediction: Human (99.91%)\n",
      "File: site_105.html | Prediction: Human (99.98%)\n",
      "File: site_87.html | Prediction: Human (84.92%)\n",
      "File: site_68.html | Prediction: Human (99.98%)\n",
      "File: site_152.html | Prediction: Human (99.82%)\n",
      "File: site_129.html | Prediction: Human (99.98%)\n",
      "File: site_44.html | Prediction: Human (99.96%)\n",
      "File: site_191.html | Prediction: Human (66.39%)\n",
      "File: site_52.html | Prediction: Human (99.98%)\n",
      "File: site_187.html | Prediction: AI (94.58%)\n",
      "File: site_168.html | Prediction: Human (98.63%)\n",
      "\n",
      "--- FINAL STATISTICS ---\n",
      "Detected as AI: 21\n",
      "Detected as Human: 140\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "def analyze_dataset(folder_path):\n",
    "    results = {\"AI\": 0, \"Human\": 0}\n",
    "    \n",
    "    # اسکن تمام فایل‌های HTML شما\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.html')]\n",
    "    print(f\"Analyzing {len(files)} files...\")\n",
    "\n",
    "    for filename in files:\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "            # استخراج متن خالص\n",
    "            text = soup.get_text()\n",
    "            # مدل‌های تشخیص معمولاً روی ۵۱۲ توکن اول بهترین جواب را می‌دهند\n",
    "            clean_text = \" \".join(text.split())\n",
    "\n",
    "            if len(clean_text) > 100: # فایل‌های خیلی کوچک را نادیده بگیر\n",
    "                prediction = detector(clean_text)[0]\n",
    "                label = \"Human\" if prediction['label'] == 'Real' else \"AI\"\n",
    "                results[label] += 1\n",
    "                print(f\"File: {filename} | Prediction: {label} ({prediction['score']:.2%})\")\n",
    "\n",
    "    print(\"\\n--- FINAL STATISTICS ---\")\n",
    "    print(f\"Detected as AI: {results['AI']}\")\n",
    "    print(f\"Detected as Human: {results['Human']}\")\n",
    "\n",
    "# مسیر پوشه‌ای که ۱۰۰ فایل در آن است را اینجا بدهید\n",
    "analyze_dataset(\"./html_dataset_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b87e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 168 files...\n",
      "File: site_92.html | Prediction: Human (99.98%)\n",
      "File: site_147.html | Prediction: Human (98.56%)\n",
      "File: site_110.html | Prediction: Human (98.67%)\n",
      "File: site_84.html | Prediction: Human (99.98%)\n",
      "File: site_151.html | Prediction: Human (99.98%)\n",
      "File: site_10.html | Prediction: Human (99.97%)\n",
      "File: site_47.html | Prediction: Human (99.93%)\n",
      "File: site_51.html | Prediction: AI (53.64%)\n",
      "File: site_184.html | Prediction: Human (99.30%)\n",
      "File: site_71.html | Prediction: Human (99.98%)\n",
      "File: site_26.html | Prediction: Human (96.75%)\n",
      "File: site_67.html | Prediction: Human (98.96%)\n",
      "File: site_126.html | Prediction: Human (95.90%)\n",
      "File: site_171.html | Prediction: Human (99.98%)\n",
      "File: site_167.html | Prediction: Human (99.73%)\n",
      "File: site_188.html | Prediction: AI (88.38%)\n",
      "File: site_130.html | Prediction: Human (99.91%)\n",
      "File: site_131.html | Prediction: Human (98.75%)\n",
      "File: site_166.html | Prediction: Human (99.88%)\n",
      "File: site_170.html | Prediction: Human (99.98%)\n",
      "File: site_127.html | Prediction: Human (99.98%)\n",
      "File: site_66.html | Prediction: Human (99.98%)\n",
      "File: site_5.html | Prediction: AI (88.16%)\n",
      "File: site_89.html | Prediction: AI (50.17%)\n",
      "File: site_31.html | Prediction: AI (99.92%)\n",
      "File: site_27.html | Prediction: Human (96.75%)\n",
      "File: site_185.html | Prediction: Human (99.98%)\n",
      "File: site_50.html | Prediction: Human (99.92%)\n",
      "File: site_193.html | Prediction: Human (99.62%)\n",
      "File: site_46.html | Prediction: Human (99.93%)\n",
      "File: site_150.html | Prediction: Human (83.46%)\n",
      "File: site_85.html | Prediction: Human (99.98%)\n",
      "File: site_111.html | Prediction: Human (99.98%)\n",
      "File: site_146.html | Prediction: Human (99.51%)\n",
      "File: site_93.html | Prediction: Human (99.92%)\n",
      "File: site_77.html | Prediction: Human (99.97%)\n",
      "File: site_98.html | Prediction: Human (95.99%)\n",
      "File: site_36.html | Prediction: Human (99.62%)\n",
      "File: site_61.html | Prediction: Human (99.92%)\n",
      "File: site_120.html | Prediction: Human (74.14%)\n",
      "File: site_177.html | Prediction: Human (99.97%)\n",
      "File: site_198.html | Prediction: Human (98.63%)\n",
      "File: site_161.html | Prediction: Human (99.84%)\n",
      "File: site_136.html | Prediction: Human (99.97%)\n",
      "File: site_94.html | Prediction: Human (99.62%)\n",
      "File: site_141.html | Prediction: AI (99.92%)\n",
      "File: site_116.html | Prediction: Human (95.99%)\n",
      "File: site_82.html | Prediction: Human (51.64%)\n",
      "File: site_157.html | Prediction: Human (99.98%)\n",
      "File: site_41.html | Prediction: Human (94.28%)\n",
      "File: site_194.html | Prediction: Human (99.22%)\n",
      "File: site_57.html | Prediction: Human (99.98%)\n",
      "File: site_182.html | Prediction: Human (98.96%)\n",
      "File: site_183.html | Prediction: Human (99.96%)\n",
      "File: site_56.html | Prediction: Human (99.92%)\n",
      "File: site_195.html | Prediction: Human (99.59%)\n",
      "File: site_40.html | Prediction: Human (99.98%)\n",
      "File: site_156.html | Prediction: Human (99.97%)\n",
      "File: site_83.html | Prediction: Human (98.56%)\n",
      "File: site_101.html | Prediction: Human (99.97%)\n",
      "File: site_117.html | Prediction: Human (99.98%)\n",
      "File: site_140.html | Prediction: AI (99.92%)\n",
      "File: site_137.html | Prediction: Human (75.63%)\n",
      "File: site_176.html | Prediction: Human (99.98%)\n",
      "File: site_199.html | Prediction: Human (99.98%)\n",
      "File: site_121.html | Prediction: Human (75.63%)\n",
      "File: site_60.html | Prediction: Human (99.98%)\n",
      "File: site_3.html | Prediction: AI (99.98%)\n",
      "File: site_37.html | Prediction: Human (99.62%)\n",
      "File: site_76.html | Prediction: Human (99.94%)\n",
      "File: site_99.html | Prediction: Human (95.99%)\n",
      "File: site_134.html | Prediction: AI (99.77%)\n",
      "File: site_59.html | Prediction: Human (97.63%)\n",
      "File: site_175.html | Prediction: Human (98.43%)\n",
      "File: site_122.html | Prediction: Human (99.98%)\n",
      "File: site_63.html | Prediction: Human (99.87%)\n",
      "File: site_159.html | Prediction: Human (98.56%)\n",
      "File: site_34.html | Prediction: Human (99.17%)\n",
      "File: site_118.html | Prediction: Human (99.98%)\n",
      "File: site_75.html | Prediction: Human (99.97%)\n",
      "File: site_138.html | Prediction: Human (99.41%)\n",
      "File: site_180.html | Prediction: Human (95.49%)\n",
      "File: site_55.html | Prediction: Human (99.58%)\n",
      "File: site_196.html | Prediction: Human (99.62%)\n",
      "File: site_179.html | Prediction: Human (99.98%)\n",
      "File: site_43.html | Prediction: Human (99.12%)\n",
      "File: site_155.html | Prediction: AI (81.78%)\n",
      "File: site_80.html | Prediction: Human (99.98%)\n",
      "File: site_102.html | Prediction: Human (99.98%)\n",
      "File: site_38.html | Prediction: AI (74.66%)\n",
      "File: site_114.html | Prediction: Human (99.82%)\n",
      "File: site_96.html | Prediction: AI (99.77%)\n",
      "File: site_79.html | Prediction: Human (99.98%)\n",
      "File: site_97.html | Prediction: AI (50.17%)\n",
      "File: site_78.html | Prediction: Human (99.47%)\n",
      "File: site_142.html | Prediction: Human (99.88%)\n",
      "File: site_81.html | Prediction: Human (99.98%)\n",
      "File: site_154.html | Prediction: Human (99.98%)\n",
      "File: site_42.html | Prediction: Human (99.65%)\n",
      "File: site_197.html | Prediction: Human (96.57%)\n",
      "File: site_178.html | Prediction: AI (88.11%)\n",
      "File: site_54.html | Prediction: Human (99.90%)\n",
      "File: site_181.html | Prediction: Human (99.98%)\n",
      "File: site_139.html | Prediction: Human (98.68%)\n",
      "File: site_74.html | Prediction: AI (93.09%)\n",
      "File: site_119.html | Prediction: Human (99.98%)\n",
      "File: site_35.html | Prediction: Human (99.93%)\n",
      "File: site_158.html | Prediction: Human (99.98%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_dataset(folder_path):\n",
    "    results = {\"AI\": 0, \"Human\": 0}\n",
    "    \n",
    "    # اسکن تمام فایل‌های HTML شما\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.html')]\n",
    "    print(f\"Analyzing {len(files)} files...\")\n",
    "\n",
    "    for filename in files:\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "            # استخراج متن خالص\n",
    "            text = soup.get_text()\n",
    "            # مدل‌های تشخیص معمولاً روی ۵۱۲ توکن اول بهترین جواب را می‌دهند\n",
    "            clean_text = \" \".join(text.split())[:1000]\n",
    "\n",
    "            if len(clean_text) > 100: # فایل‌های خیلی کوچک را نادیده بگیر\n",
    "                prediction = detector(clean_text)[0]\n",
    "                label = \"Human\" if prediction['label'] == 'Real' else \"AI\"\n",
    "                results[label] += 1\n",
    "                print(f\"File: {filename} | Prediction: {label} ({prediction['score']:.2%})\")\n",
    "\n",
    "    print(\"\\n--- FINAL STATISTICS ---\")\n",
    "    print(f\"Detected as AI: {results['AI']}\")\n",
    "    print(f\"Detected as Human: {results['Human']}\")\n",
    "\n",
    "# مسیر پوشه‌ای که ۱۰۰ فایل در آن است را اینجا بدهید\n",
    "analyze_dataset(\"./html_dataset_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814737e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyMuPDF\n",
      "  Downloading pymupdf-1.26.7-cp310-abi3-macosx_11_0_arm64.whl (22.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pyMuPDF\n",
      "Successfully installed pyMuPDF-1.26.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d22874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "\n",
    "# استفاده از یک مدل چندزبانه که برای تشخیص متن (Fake/Real) منعطف‌تر است\n",
    "# نکته: مدل‌های عمومی فقط ویژگی‌های زبانی را استخراج می‌کنند. \n",
    "# برای تشخیص دقیق \"AI vs Human\" فارسی، مدل تخصصی در HuggingFace محدود است.\n",
    "detector = pipeline(\"text-classification\", model=\"xlm-roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9e885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 8 files...\n",
      "File: CAR_NB_Kiana_Nasiri.pdf | Prediction: Human (57.69%)\n",
      "File: MS_APRIORI_Kiana_Nasiri.pdf | Prediction: Human (57.69%)\n",
      "File: ARM.pdf | Prediction: Human (57.69%)\n",
      "File: Tic_Toc_Toy_Kiana_Nasiri.pdf | Prediction: Human (57.69%)\n",
      "File: ARM_Kiana_Nasiri.pdf | Prediction: Human (57.69%)\n",
      "File: AI_History.pdf | Prediction: Human (57.69%)\n",
      "File: N_Queen_Kiana_Nasiri.pdf | Prediction: Human (57.69%)\n",
      "File: Page_Kiana_Nasiri.pdf | Prediction: Human (57.69%)\n",
      "\n",
      "--- FINAL STATISTICS ---\n",
      "Detected as AI: 0\n",
      "Detected as Human: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_dataset(folder_path):\n",
    "    results = {\"AI\": 0, \"Human\": 0}\n",
    "    \n",
    "    # پیدا کردن تمام فایل‌های PDF\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    print(f\"Analyzing {len(files)} files...\")\n",
    "    \n",
    "    for filename in files:\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        text = \"\"\n",
    "        try:\n",
    "            # استخراج متن از PDF\n",
    "            doc = fitz.open(path)\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            doc.close()\n",
    "            \n",
    "            # پاکسازی و محدود کردن طول متن (مدل‌ها تا ۵۱۲ توکن را می‌پذیرند)\n",
    "            clean_text = \" \".join(text.split())[:1000]\n",
    "            clean_text = text[:1000]\n",
    "            # بخش تشخیص (داخل بلاک try و با شرط طول متن)\n",
    "            if len(clean_text) > 100:\n",
    "                prediction = detector(clean_text)[0]\n",
    "                \n",
    "                # نکته فنی: چون این مدل‌ها روی لیبل‌های مختلفی آموزش دیده‌اند، \n",
    "                # ما بر اساس Score یا Label احتمالی تصمیم می‌گیریم.\n",
    "                # در اکثر دیتکتورها LABEL_0 انسان و LABEL_1 هوش مصنوعی است.\n",
    "                label_id = prediction['label']\n",
    "                score = prediction['score']\n",
    "                \n",
    "                # این یک شبیه‌سازی منطقی برای پروژه شماست:\n",
    "                label = \"AI\" if label_id == 'LABEL_1' else \"Human\"\n",
    "                \n",
    "                results[label] += 1\n",
    "                print(f\"File: {filename} | Prediction: {label} ({score:.2%})\")\n",
    "            else:\n",
    "                print(f\"File: {filename} | Error: Text too short or empty\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n--- FINAL STATISTICS ---\")\n",
    "    print(f\"Detected as AI: {results['AI']}\")\n",
    "    print(f\"Detected as Human: {results['Human']}\")\n",
    "\n",
    "# مسیر پوشه RAG را بررسی کنید که درست باشد\n",
    "analyze_dataset(\"/Users/kiananasiri/Documents/+UNI_Terms/Term6/ML/Assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854fb909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
